{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGAN",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RFhE6PtFs-y",
        "colab_type": "code",
        "outputId": "9d615b7a-cfbe-46c1-a360-e393f58d51a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.layers import Activation,Reshape,Conv2D,Dense,Flatten,BatchNormalization,Dropout,UpSampling2D,Input,Conv2DTranspose,LeakyReLU,ReLU\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model,Sequential\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "or1ybcbbFs7c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_rows = 28\n",
        "img_cols = 28\n",
        "channels = 1\n",
        "img_shape = (img_rows, img_cols, channels)\n",
        "latent_dim = 100\n",
        "\n",
        "optimizer = Adam(0.0002, 0.5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRzzm5-YDkUB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_generator():\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Dense(7*7*128, input_dim=latent_dim))\n",
        "        model.add(Reshape((7, 7, 128)))\n",
        "        \n",
        "        model.add(Conv2DTranspose(128, kernel_size=5, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.5))\n",
        "        model.add(ReLU())\n",
        "        \n",
        "        \n",
        "        model.add(Conv2DTranspose(64, kernel_size=5, padding=\"same\",strides=2))\n",
        "        model.add(BatchNormalization(momentum=0.5))\n",
        "        model.add(ReLU())\n",
        "        \n",
        "        model.add(Conv2DTranspose(64, kernel_size=5, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.5))\n",
        "        model.add(ReLU())\n",
        "        \n",
        "        model.add(Conv2DTranspose(1, kernel_size=5, padding=\"same\",strides=2))\n",
        "        model.add(BatchNormalization(momentum=0.5))\n",
        "        model.add(Activation(\"tanh\"))\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        noise = Input(shape=(latent_dim,))\n",
        "        img = model(noise)\n",
        "\n",
        "        return Model(noise, img)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8miY5oLDkRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_discriminator():\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(64, kernel_size=5, strides=2, input_shape=img_shape, padding=\"same\"))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "    model.add(Conv2D(128, kernel_size=5, strides=2, padding=\"same\"))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Conv2D(256, kernel_size=5, strides=1, padding=\"same\"))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    img = Input(shape=img_shape)\n",
        "    validity = model(img)\n",
        "\n",
        "    return Model(img, validity)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cd3EOkfvFI_s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def combined():\n",
        "        # The generator takes noise as input and generates imgs\n",
        "        z = Input(shape=(latent_dim,))\n",
        "        gentd_img = generator(z)\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        discriminator.trainable = False\n",
        "\n",
        "        # The discriminator takes generated images as input and determines validity\n",
        "        valid = discriminator(gentd_img)\n",
        "\n",
        "        # The combined model  (stacked generator and discriminator)\n",
        "        # Trains the generator to fool the discriminator\n",
        "        combined = Model(z, valid)\n",
        "        combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "        \n",
        "        return combined"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wluPCs1nDkPF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epochs, batch_size=128, save_interval=50):\n",
        "\n",
        "    # Load the dataset\n",
        "    (X_train, _), (_, _) = mnist.load_data()\n",
        "\n",
        "    # Rescale -1 to 1\n",
        "    X_train = X_train / 127.5 - 1.\n",
        "    X_train = np.expand_dims(X_train, axis=3)\n",
        "\n",
        "    # Adversarial ground truths\n",
        "    valid = np.ones((batch_size, 1))\n",
        "    fake = np.zeros((batch_size, 1))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        #  Train Discriminator\n",
        "\n",
        "        # Select a random half of images\n",
        "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "        imgs = X_train[idx]\n",
        "\n",
        "        # Sample noise and generate a batch of new images\n",
        "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "        gen_imgs = generator.predict(noise)\n",
        "\n",
        "        # Train the discriminator (real classified as ones and generated as zeros)\n",
        "        \n",
        "        d_loss_real = discriminator.train_on_batch(imgs, valid)\n",
        "        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "        #  Train Generator\n",
        "\n",
        "        # Train the generator (wants discriminator to mistake images as real)\n",
        "        g_loss = gan.train_on_batch(noise, valid)\n",
        "\n",
        "        # Display the progress after every 200 epoch\n",
        "        if epoch%200 == 0 :\n",
        "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "        \n",
        "        # If at save interval => save generated image samples\n",
        "        if epoch % save_interval == 0:\n",
        "            # Plot the progress\n",
        "                \n",
        "            save_imgs(epoch)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2i6Jj5RDkNG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_imgs(epoch):\n",
        "    r, c = 3, 3\n",
        "    noise = np.random.normal(0, 1, (r * c, latent_dim))\n",
        "    gen_imgs = generator.predict(noise)\n",
        "\n",
        "    # Rescale images 0 - 1\n",
        "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "    fig, axs = plt.subplots(r, c)\n",
        "    cnt = 0\n",
        "    for i in range(r):\n",
        "        for j in range(c):\n",
        "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "            axs[i,j].axis('off')\n",
        "            cnt += 1\n",
        "    fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
        "    plt.close()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7D4jDGsDkKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir  images/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sg70L04nDkHh",
        "colab_type": "code",
        "outputId": "83eb5e2d-8cfc-4bc9-dc9f-cfa30447d96a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "generator =  build_generator()\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])    \n",
        "gan = combined()\n",
        "train(epochs=20001, batch_size=32, save_interval=1000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 6272)              633472    \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 7, 7, 128)         409728    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "re_lu_1 (ReLU)               (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTr (None, 14, 14, 64)        204864    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "re_lu_2 (ReLU)               (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTr (None, 14, 14, 64)        102464    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "re_lu_3 (ReLU)               (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_4 (Conv2DTr (None, 28, 28, 1)         1601      \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 28, 28, 1)         4         \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 28, 28, 1)         0         \n",
            "=================================================================\n",
            "Total params: 1,353,157\n",
            "Trainable params: 1,352,643\n",
            "Non-trainable params: 514\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 64)        1664      \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 7, 7, 128)         204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 7, 7, 256)         819456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 7, 7, 256)         1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 7, 7, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 7, 7, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 12544)             0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 12545     \n",
            "=================================================================\n",
            "Total params: 1,040,385\n",
            "Trainable params: 1,039,489\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 [D loss: 1.794272, acc.: 37.50%] [G loss: 0.609590]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "200 [D loss: 0.007873, acc.: 100.00%] [G loss: 6.654810]\n",
            "400 [D loss: 0.032928, acc.: 100.00%] [G loss: 5.900127]\n",
            "600 [D loss: 0.052406, acc.: 96.88%] [G loss: 7.205486]\n",
            "800 [D loss: 0.001697, acc.: 100.00%] [G loss: 10.051389]\n",
            "1000 [D loss: 0.019079, acc.: 100.00%] [G loss: 6.585712]\n",
            "1200 [D loss: 0.141725, acc.: 95.31%] [G loss: 3.802435]\n",
            "1400 [D loss: 0.050706, acc.: 96.88%] [G loss: 10.477589]\n",
            "1600 [D loss: 0.032284, acc.: 100.00%] [G loss: 5.375966]\n",
            "1800 [D loss: 0.054577, acc.: 100.00%] [G loss: 6.003273]\n",
            "2000 [D loss: 0.122860, acc.: 95.31%] [G loss: 9.223886]\n",
            "2200 [D loss: 0.130520, acc.: 98.44%] [G loss: 6.168232]\n",
            "2400 [D loss: 0.048609, acc.: 100.00%] [G loss: 5.348415]\n",
            "2600 [D loss: 0.004656, acc.: 100.00%] [G loss: 6.999157]\n",
            "2800 [D loss: 0.132426, acc.: 95.31%] [G loss: 5.258495]\n",
            "3000 [D loss: 0.056960, acc.: 98.44%] [G loss: 7.778897]\n",
            "3200 [D loss: 0.126331, acc.: 98.44%] [G loss: 4.849991]\n",
            "3400 [D loss: 0.026229, acc.: 100.00%] [G loss: 4.945498]\n",
            "3600 [D loss: 0.637745, acc.: 64.06%] [G loss: 3.035362]\n",
            "3800 [D loss: 0.073053, acc.: 98.44%] [G loss: 3.675543]\n",
            "4000 [D loss: 0.087673, acc.: 96.88%] [G loss: 5.117333]\n",
            "4200 [D loss: 1.315152, acc.: 31.25%] [G loss: 7.055843]\n",
            "4400 [D loss: 0.087058, acc.: 98.44%] [G loss: 4.008659]\n",
            "4600 [D loss: 0.274782, acc.: 85.94%] [G loss: 3.038558]\n",
            "4800 [D loss: 0.191314, acc.: 95.31%] [G loss: 4.584853]\n",
            "5000 [D loss: 0.079237, acc.: 100.00%] [G loss: 2.419578]\n",
            "5200 [D loss: 0.236023, acc.: 92.19%] [G loss: 2.173000]\n",
            "5400 [D loss: 0.509065, acc.: 78.12%] [G loss: 3.046857]\n",
            "5600 [D loss: 0.433706, acc.: 81.25%] [G loss: 3.521618]\n",
            "5800 [D loss: 0.768183, acc.: 54.69%] [G loss: 4.041220]\n",
            "6000 [D loss: 0.281701, acc.: 92.19%] [G loss: 3.317547]\n",
            "6200 [D loss: 0.326728, acc.: 85.94%] [G loss: 3.024513]\n",
            "6400 [D loss: 0.295622, acc.: 90.62%] [G loss: 2.125783]\n",
            "6600 [D loss: 0.465071, acc.: 76.56%] [G loss: 3.777419]\n",
            "6800 [D loss: 0.107880, acc.: 98.44%] [G loss: 3.885606]\n",
            "7000 [D loss: 0.359563, acc.: 79.69%] [G loss: 3.449828]\n",
            "7200 [D loss: 0.216246, acc.: 96.88%] [G loss: 3.009611]\n",
            "7400 [D loss: 0.342466, acc.: 84.38%] [G loss: 3.560822]\n",
            "7600 [D loss: 0.628971, acc.: 62.50%] [G loss: 3.800216]\n",
            "7800 [D loss: 0.512745, acc.: 75.00%] [G loss: 2.732707]\n",
            "8000 [D loss: 0.842940, acc.: 57.81%] [G loss: 1.715880]\n",
            "8200 [D loss: 0.165048, acc.: 96.88%] [G loss: 2.158009]\n",
            "8400 [D loss: 1.125711, acc.: 34.38%] [G loss: 2.306296]\n",
            "8600 [D loss: 0.403147, acc.: 79.69%] [G loss: 1.739003]\n",
            "8800 [D loss: 0.382844, acc.: 79.69%] [G loss: 1.966319]\n",
            "9000 [D loss: 0.711073, acc.: 62.50%] [G loss: 1.798455]\n",
            "9200 [D loss: 1.170607, acc.: 28.12%] [G loss: 1.456319]\n",
            "9400 [D loss: 0.551775, acc.: 76.56%] [G loss: 1.955350]\n",
            "9600 [D loss: 0.436812, acc.: 81.25%] [G loss: 1.827083]\n",
            "9800 [D loss: 0.482803, acc.: 76.56%] [G loss: 2.173257]\n",
            "10000 [D loss: 0.574321, acc.: 67.19%] [G loss: 2.116437]\n",
            "10200 [D loss: 0.607530, acc.: 67.19%] [G loss: 1.504408]\n",
            "10400 [D loss: 0.703689, acc.: 59.38%] [G loss: 2.308614]\n",
            "10600 [D loss: 0.872870, acc.: 39.06%] [G loss: 2.340447]\n",
            "10800 [D loss: 0.959330, acc.: 39.06%] [G loss: 2.037522]\n",
            "11000 [D loss: 0.804736, acc.: 51.56%] [G loss: 1.205287]\n",
            "11200 [D loss: 0.801352, acc.: 50.00%] [G loss: 1.762895]\n",
            "11400 [D loss: 0.747748, acc.: 53.12%] [G loss: 1.579470]\n",
            "11600 [D loss: 0.482358, acc.: 82.81%] [G loss: 1.456725]\n",
            "11800 [D loss: 1.157972, acc.: 31.25%] [G loss: 1.312079]\n",
            "12000 [D loss: 0.741559, acc.: 48.44%] [G loss: 1.559928]\n",
            "12200 [D loss: 0.657381, acc.: 65.62%] [G loss: 1.585165]\n",
            "12400 [D loss: 0.806527, acc.: 51.56%] [G loss: 1.633212]\n",
            "12600 [D loss: 0.812420, acc.: 51.56%] [G loss: 1.707094]\n",
            "12800 [D loss: 0.483093, acc.: 81.25%] [G loss: 1.258804]\n",
            "13000 [D loss: 0.700475, acc.: 59.38%] [G loss: 1.543072]\n",
            "13200 [D loss: 0.691558, acc.: 57.81%] [G loss: 1.553527]\n",
            "13400 [D loss: 0.770730, acc.: 54.69%] [G loss: 1.115179]\n",
            "13600 [D loss: 0.722695, acc.: 57.81%] [G loss: 1.239697]\n",
            "13800 [D loss: 0.710302, acc.: 59.38%] [G loss: 1.422814]\n",
            "14000 [D loss: 0.908069, acc.: 40.62%] [G loss: 1.161848]\n",
            "14200 [D loss: 0.779734, acc.: 56.25%] [G loss: 1.418674]\n",
            "14400 [D loss: 0.894666, acc.: 43.75%] [G loss: 1.159323]\n",
            "14600 [D loss: 0.717834, acc.: 59.38%] [G loss: 1.128788]\n",
            "14800 [D loss: 0.535656, acc.: 78.12%] [G loss: 1.647462]\n",
            "15000 [D loss: 0.825429, acc.: 48.44%] [G loss: 1.450189]\n",
            "15200 [D loss: 0.729817, acc.: 54.69%] [G loss: 1.694277]\n",
            "15400 [D loss: 0.752147, acc.: 48.44%] [G loss: 1.173204]\n",
            "15600 [D loss: 0.778469, acc.: 53.12%] [G loss: 1.501532]\n",
            "15800 [D loss: 0.557171, acc.: 73.44%] [G loss: 1.408496]\n",
            "16000 [D loss: 0.665609, acc.: 64.06%] [G loss: 1.090136]\n",
            "16200 [D loss: 0.602743, acc.: 67.19%] [G loss: 1.452384]\n",
            "16400 [D loss: 0.871240, acc.: 48.44%] [G loss: 1.351034]\n",
            "16600 [D loss: 0.722670, acc.: 54.69%] [G loss: 1.410909]\n",
            "16800 [D loss: 0.692212, acc.: 59.38%] [G loss: 1.846660]\n",
            "17000 [D loss: 0.727407, acc.: 60.94%] [G loss: 1.966731]\n",
            "17200 [D loss: 0.816605, acc.: 48.44%] [G loss: 1.169609]\n",
            "17400 [D loss: 0.639916, acc.: 62.50%] [G loss: 1.590705]\n",
            "17600 [D loss: 0.558887, acc.: 73.44%] [G loss: 1.195612]\n",
            "17800 [D loss: 0.662435, acc.: 59.38%] [G loss: 1.249950]\n",
            "18000 [D loss: 0.772529, acc.: 50.00%] [G loss: 1.586898]\n",
            "18200 [D loss: 0.576787, acc.: 65.62%] [G loss: 1.510839]\n",
            "18400 [D loss: 0.626578, acc.: 62.50%] [G loss: 1.805933]\n",
            "18600 [D loss: 0.640385, acc.: 60.94%] [G loss: 1.210492]\n",
            "18800 [D loss: 0.647081, acc.: 64.06%] [G loss: 1.612468]\n",
            "19000 [D loss: 0.622181, acc.: 62.50%] [G loss: 1.881974]\n",
            "19200 [D loss: 0.619806, acc.: 62.50%] [G loss: 1.202272]\n",
            "19400 [D loss: 0.552647, acc.: 75.00%] [G loss: 0.970603]\n",
            "19600 [D loss: 0.547264, acc.: 73.44%] [G loss: 1.453871]\n",
            "19800 [D loss: 0.786106, acc.: 43.75%] [G loss: 1.350560]\n",
            "20000 [D loss: 0.694275, acc.: 57.81%] [G loss: 1.240658]\n",
            "CPU times: user 16min 32s, sys: 4min 12s, total: 20min 45s\n",
            "Wall time: 20min 42s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KiZcGhIDkFh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfoXgtuKDkDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKL4zupbDj_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMT5Xq4kDj7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}